{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Sentence': ['Hello, how are you?', 'I am learning about AI!', 'Transformers are interesting.']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"sentences.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n",
    "\n",
    "## Tokenization in Natural Language Processing (NLP)\n",
    "\n",
    "Tokenization in Natural Language Processing (NLP) is the process of breaking down text into smaller units, called tokens. These tokens can be words, subwords, characters, or parts of words. Tokenization transforms unstructured text into a format that can be easily processed by algorithms.\n",
    "\n",
    "### Types of Tokenization\n",
    "\n",
    "1. **Word Tokenization**: This involves splitting text into individual words.\n",
    "   - Example: \"Hello world\" becomes [\"Hello\", \"world\"]\n",
    "\n",
    "2. **Subword Tokenization**: Often used in advanced NLP models like BERT, it breaks words into sub-units or characters to better handle rare or unknown words.\n",
    "   - Example: \"smarter\" might be tokenized as [\"smart\", \"er\"]\n",
    "\n",
    "3. **Character Tokenization**: This approach splits text into individual characters, useful for certain types of linguistic analysis or languages without clear word boundaries.\n",
    "   - Example: \"cat\" becomes [\"c\", \"a\", \"t\"]\n",
    "\n",
    "4. **Sentence Tokenization**: This method breaks text into individual sentences, often used for tasks that require understanding the context of whole sentences.\n",
    "   - Example: \"Hello world. It's a great day.\" becomes [\"Hello world.\", \"It's a great day.\"]\n",
    "\n",
    "### Purpose and Importance\n",
    "\n",
    "- **Machine Readability**: Tokenization converts text into a form that is easier for machines to understand and process.\n",
    "- **Simplification of Text Analysis**: By breaking text into smaller parts, tokenization simplifies complex natural language processing tasks like parsing and sentiment analysis.\n",
    "- **Handling Ambiguity and Context**: It helps in understanding the context and meaning of text, especially important in languages with complex grammar and syntax.\n",
    "\n",
    "### Example in Context\n",
    "\n",
    "Consider the sentence \"NLP stands for Natural Language Processing.\" When tokenized, it becomes:\n",
    "\n",
    "```python\n",
    "[\"NLP\", \"stands\", \"for\", \"Natural\", \"Language\", \"Processing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy model for tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Tokenization function using spacy -> The function returns a list of lowercase tokens.\n",
    "def tokenize(text):\n",
    "    return [token.text.lower() for token in nlp.tokenizer(text)]\n",
    "\n",
    "# Create a simple dataset\n",
    "data = {\n",
    "    'Sentence': ['Hello, how are you?', 'I am learning about AI!', 'Transformers are interesting.']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"sentences.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the vocabulary\n",
    "\n",
    "`build_vocab` is a function that takes a DataFrame as input.\n",
    "\n",
    "It initializes an empty dictionary vocab, which will store each unique token as a key and its corresponding index as a value.\n",
    "The function iterates over each sentence in the DataFrame, tokenizes it, and then iterates over each token.\n",
    "\n",
    "For each token, if it is not already in vocab, it is added to vocab with a value that is the current length of vocab. This means each token gets a unique index.\n",
    "\n",
    "The function returns the vocab dictionary, which represents the vocabulary built from the dataset.\n",
    "\n",
    "**vocab is a dictionary representing the vocabulary (mapping of tokens to indices).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(dataframe):\n",
    "    vocab = {}\n",
    "    for sentence in dataframe['Sentence']:\n",
    "        tokens = tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0,\n",
       " ',': 1,\n",
       " 'how': 2,\n",
       " 'are': 3,\n",
       " 'you': 4,\n",
       " '?': 5,\n",
       " 'i': 6,\n",
       " 'am': 7,\n",
       " 'learning': 8,\n",
       " 'about': 9,\n",
       " 'ai': 10,\n",
       " '!': 11,\n",
       " 'transformers': 12,\n",
       " 'interesting': 13,\n",
       " '.': 14}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, vocab):\n",
    "        self.dataframe = dataframe\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx]['Sentence']\n",
    "        print(text)\n",
    "        return torch.tensor([self.vocab[token] for token in tokenize(text)], dtype=torch.long)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(df, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each item in the dataset is a tensor representing **a sequence of indices corresponding to the tokens of a sentence from the DataFrame**, which is a common format for NLP tasks in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df.iloc[0]['Sentence']\n",
    "[vocab[token] for token in tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_length):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embeddings = nn.Embedding(max_length, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(x.size(0)).unsqueeze(0)\n",
    "        return self.token_embeddings(x) + self.position_embeddings(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "embedding_dim = 64\n",
    "vocab_size = len(vocab)\n",
    "max_length = max([len(tokenize(sentence)) for sentence in df['Sentence']])\n",
    "model = TransformerEmbedding(vocab_size, embedding_dim, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are interesting.\n",
      "torch.Size([1, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example input and embedding extraction\n",
    "for batch in dataloader:\n",
    "    embeddings = model(batch[0])\n",
    "    print(embeddings.shape)\n",
    "    break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
